"[Cryptocurrency Price Prediction Challenge](https://bitgrit.net/competition/20)"

*   This project was very interesting and came with lots of trials and tribulations. We tried many different variations of feature engineering, imputation methods, and dimensionality reduction. At the end of the project, we decided to keep it simple and just impute zeros for all null values across our dataset. This approach led us to the best result. During this project we also almost made the mistake of introducing data leakage. By first splitting the dataframe into sequences and then into train and test we ensured that our model was free of information from the test holdout data.

*   Overall we struggled with obtaining a high weighted average f1 score on test. Some of our initial models were overfitting on train and predicting only 1 of our classes for Target on test. To combat this we tried to make our model architecture simple, adding one SimpleRNN input layer with recurrent dropout, into a Dropout Layer (0.2) and finally into the final output dense layer. We utilized the adam optimizer and binary crossentropy for loss. Our input layer had 70 features, closs to the n_features of 76. All of these aspects in conjunction reduced the test predictions from predicting only one class and gave us the highest weighted avg f1 score of 0.67 out of any of our prior models.

* After experimenting with various preprocessing methods, including feature importance, PCA, and different imputation techniques, we observed no improvement in performance. Surprisingly, PCA even resulted in a decrease in the F1 score by 0.2 points. Initially, these methods were expected to enhance prediction performance; however, their implementation did not yield the anticipated results. This outcome highlights the complexity of the dataset and suggests that alternative approaches may be necessary to achieve performance improvements.

* We successfully implemented a robust preprocessing pipeline that effectively transformed the dataset, addressing missing values with zero imputation based on its positive impact on model performance. This approach to data preparation helped in maintaining the integrity of our sequences and safeguarded against data leakage, ensuring that our training and testing data remained mutually exclusive.

* The iterative process of model evaluation and adjustment led to an improvement in performance, with the highest weighted average F1 score 0.67 to date on the test set. This continuous cycle of testing and refinement helped us understand the limitations of our initial approaches and guided us towards a more effective model, indicating a deepening understanding of the underlying data patterns and model behavior.

# [Cryptocurrency Price Prediction Challenge](https://bitgrit.net/competition/20)

#### This project discusses the Crypto Currency Price Prediction Challenge that took place in the OPIM5509 course during the spring semester of 2024 at the University of Connecticut.

*   This project was very interesting and came with lots of trials and tribulations. I tried many different variations of feature engineering, imputation methods, and dimensionality reduction. At the end of the project, I decided to keep it simple and just impute zeros for all null values across the dataset. This approach led us to the best result. During this project I also almost made the mistake of introducing data leakage. By first splitting the dataframe into sequences and then into train and test I ensured that the model was free of information from the test holdout data.

*   Overall I struggled with obtaining a high weighted average f1 score on test. Some of the initial models were overfitting on train and predicting only 1 of the classes for Target on test. To combat this I tried to make the model architecture simple, adding one SimpleRNN input layer with recurrent dropout, into a Dropout Layer (0.2) and finally into the final output dense layer. I utilized the adam optimizer and binary crossentropy for loss. The input layer had 70 features, closs to the n_features of 76. All of these aspects in conjunction reduced the test predictions from predicting only one class and gave us the highest weighted avg f1 score of 0.67 out of any of the prior models.

* After experimenting with various preprocessing methods, including feature importance, PCA, and different imputation techniques, I observed no improvement in performance. Surprisingly, PCA even resulted in a decrease in the F1 score by 0.2 points. Initially, these methods were expected to enhance prediction performance; however, their implementation did not yield the anticipated results. This outcome highlights the complexity of the dataset and suggests that alternative approaches may be necessary to achieve performance improvements.

* I successfully implemented a robust preprocessing pipeline that effectively transformed the dataset, addressing missing values with zero imputation based on its positive impact on model performance. This approach to data preparation helped in maintaining the integrity of the sequences and safeguarded against data leakage, ensuring that the training and testing data remained mutually exclusive.

* The iterative process of model evaluation and adjustment led to an improvement in performance, with the highest weighted average F1 score 0.67 to date on the test set. This continuous cycle of testing and refinement helped us understand the limitations of the initial approaches and guided us towards a more effective model, indicating a deepening understanding of the underlying data patterns and model behavior.
